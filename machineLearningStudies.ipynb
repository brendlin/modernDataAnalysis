{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Notes on Machine Learning\n",
    "==============\n",
    "\n",
    "Areas of follow-up:\n",
    " - Investigate \"pre-training of initial embeddings with unsupervised learning methods such as autoencoders\" to speed up training (due to the \"vanishing gradient problem\" of deep neural networks)\n",
    " - Would also like to learn more about this: \"Autoencoders attempt to learn a useful layered representation of the data without having to backpropagate through a deep network; standard gradient descent is only used at the end to fine-tune the network.\"\n",
    " - Some interesting work on how you can have a parameterized classifier, that approximates a likelihood ratio, that allows you to estimate the parameters themselves using the classifier. (Baldi P, et al. Eur. Phys. J. C 76:235 (2016))\n",
    "\n",
    "kNN (kNearestNeighbor)\n",
    "=======\n",
    "\n",
    "Features:\n",
    " - Poll the $k$ nearest data points and pick the majority class.\n",
    " - **Disadvantages**: large evaluation times, very memory-intensive, does not do well on sparse data\n",
    " - **Advantages**: very simple to understand.\n",
    "\n",
    "\n",
    "Linear Models for Classification\n",
    "=======\n",
    "\n",
    "Simply put, these are models that try to classify data in a parameter space by applying linear cuts, e.g.\n",
    "\n",
    "$y(\\mathbf x) = a_1 x_1 + a_2 x_2 + .... + b > 0$\n",
    "\n",
    "where the $>0$ threshold indicates a binary classification. For a simple case, take an binary output variable that depends on 2 input variables, e.g. points in a plane. Then this classifier would draw a line in the 2D plane to try to separate the two classes as efficiently as possible.\n",
    "\n",
    "See **support vector machines** for a more complex version of this.\n",
    "\n",
    "\n",
    "Logistic Regression\n",
    "========\n",
    "\n",
    "Used when your output variable is a probability between 0 and 1 (or a multi-class probability with the sum of classes adding to 1). It uses a **logistic (sigmoid) function** to map the inputs to a probability between 0 and 1. A logistic regression can have >1 input variables.\n",
    "\n",
    "The logistic regression is really just a fit to data where y=0 or 1; it can be converted to a classifier by applying a threshold cut to the output logistic estimate.\n",
    "\n",
    "Terminology:\n",
    " - **Logit**: basically the logistic function that maps the infinite space to 0-1.\n",
    " - **Probit**: Very similar function to the logit function, but based on the CDF of a gaussian function.\n",
    " - **Odds ratio**: ??\n",
    "\n",
    "Fisher Discriminant\n",
    "========\n",
    "\n",
    "Project the data into lower dimension by (a) minimizing the in-class variance, and (b) maximizing the variance between classes.\n",
    "\n",
    "\n",
    "Naive Bayes and Likelihood Ratio\n",
    "========\n",
    "\n",
    "See statisticalMethods -- these assume that your input variables are uncorrelated.\n",
    "\n",
    "\n",
    "Support Vector Machines (SVM)\n",
    "==========\n",
    "\n",
    "A method of regression or classification that maps labeled data onto a **larger parameter space** so as to maximize the distance between two classes, and uses this mapping as a basis for classification of unlabeled data. \n",
    "\n",
    "In the simplest case, a line divides two classes in plane, or a plane divides two classes in a 3d space. By introducing **kernels** (see statisticalMethods), the boundary can assume complex shapes (which presumably are still hyperplanes in the newly-defined parameter space).\n",
    "\n",
    "The **support vectors** of the SVM are the data points at the boundary of the two different classes, which drive the definition of the boundary itself.\n",
    "\n",
    "SVMs can also be used in the context of clustering.\n",
    "\n",
    "**Scaling / normalization preprocessing** is an important step for SVMs, which otherwise do not do very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees and Forests\n",
    "========\n",
    "\n",
    "Metrics: Gini Impurity and Entropy\n",
    "-------\n",
    "\n",
    "The **Gini impurity** is a decision-tree metric; it is the (summed) probability of misclassifying a sample in a given region. For a multi-class problem, in a given region, it is given by:\n",
    "\n",
    "$I_G(p) = \\sum^J_{i=1}\\left( p_i \\sum_{k\\neq i} p_k\\right) = ... = 1 - \\sum p_k^2 $\n",
    "\n",
    "(as you can see it is one minus the square of the probability of correct assignment.)\n",
    "\n",
    "The change in **information entropy** is used to calculate splits for decision trees. The variable with the largest change in information entropy is selected to split the data in a given iteration.\n",
    "\n",
    "\n",
    "Decision Tree\n",
    "--------\n",
    "\n",
    "Find the most powerful variable and apply the most discriminating cut; in the two subsequent populations, repeat the process with the next-most-powerful variable, ... etc. You can **prune** (remove nodes at the end) or **pre-prune** (stopping the continuation of a tree).\n",
    " - **Pruning**: `max_depth`, `max_leaf_nodes`, `min_samples_leaf`\n",
    " - **Feature importance**: A ranked list of the most powerful features (a number between 0 and 1). Note that if there are two highly correlated variables then it is possible that one of the features has a lower feature imporantance than you might think.\n",
    " - **No preprocessing necessary**: Normalization and standardization are not required!\n",
    " \n",
    "\n",
    "Random Forest (a case of Bootstrap Aggregation or BAgging)\n",
    "---------\n",
    "\n",
    "To correct for **over-fitting** in a single decision tree, create an ensemble of trees and insert some randomness into their creation (drop data points; remove variables). Then average the collection of results.\n",
    "\n",
    " - `n_estimators`: Number of trees\n",
    " - `max_features`: Maximum features to look at in the dataset, for a given tree.\n",
    "\n",
    "Boosted Decision Tree (e.g. Gradient Boosted)\n",
    "---------\n",
    "\n",
    " - **Not good for sparse, high-dimensional data**\n",
    " - Generally faster execution than random forests\n",
    " - `learning_rate`: steepness of the gradient descent.\n",
    " - `n_estimators`: Number of trees\n",
    " - `max_depth` (of individual trees): Usually keep this at 5 or lower.\n",
    "\n",
    "AdaBoost\n",
    "----------\n",
    "\n",
    "Achieves the boosting by means of increasing the **weight** of misclassified events to boost their importance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks\n",
    "==========\n",
    "\n",
    "The Big Summary:\n",
    " - **Feed-forward**: The normal-looking neural network\n",
    " - **Radial basis function**: Same as feed-forward, but with an RBF activation function.\n",
    " - **Recurrent NN**: some time-ordering (or other ordering) functionailty by which nodes are connected to k+1th sample. Subject to the vanishing (exploding) gradient problem.\n",
    " - **LSTM**: an RNN with **gates** and a **memory cell**. Tries to solve the vanishing/exploding gradient issue. Resource-intensive.\n",
    " - **Gated recurrent units (GRU)**: Similar to LSTM, but with an \"update gate\" instead of an input/output/forget gate.\n",
    " - **Autoencoders**: Encode (compress) info. Good for dimensionality reduction, de-noising some things, etc.\n",
    " - **GAN**: Generative adversarial network. Two networks compete, one trying to generate new data that looks like the training data, and the other trying to discriminate between the real and simulated data.\n",
    "\n",
    "\n",
    "**Terminology**:\n",
    " - `'softmax'`: Final layer for multi-class output\n",
    " - `'sigmoid'`: Final layer for binary classification output\n",
    "\n",
    "Multilayer Perceptron (MLP)\n",
    "---------\n",
    "\n",
    "A combination of **hidden layers** connected by **weights**, linearly combining inputs and applying an **activation function**.\n",
    "\n",
    "Activation functions:\n",
    " - **Rectified Linear Unit (RELU)**\n",
    " - **tanh** (hyperbolic tangent)\n",
    "\n",
    "**Regularization**: you can regularize a NN by constraining the weights to be close to 0.<br>\n",
    "**Scaling**: Neural nets work better if you scale / normalize the data.\n",
    "\n",
    "\n",
    "\n",
    "Convolutional Neural Networks\n",
    "----------\n",
    " - Convolutional architecture: a filter window is swept over the (spatially-related) input\n",
    " - Pooling: that filtered information is summarized (e.g. takes the maximum or the average)\n",
    " - Normalization Layers\n",
    " - Residual Layers\n",
    " - Graph convolutional networks (as opposed to tree networks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks\n",
    "-----------\n",
    "\n",
    " - long-short-term-memory (LSTM)\n",
    " - Gated recurrent unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adversarial Neural Networks\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation and Metrics\n",
    "============\n",
    "\n",
    "Loss Functions\n",
    "--------\n",
    " - Hinge Loss (???)\n",
    " - See tensorflowPrimer for others\n",
    "\n",
    "\n",
    "Metrics\n",
    "-------\n",
    " - **F1**: The **harmonic mean** of the precision and recall\n",
    " - **Accuracy**:\n",
    " - **Confusion Matrix**:\n",
    "\n",
    "\n",
    "Cross-validation\n",
    "-------\n",
    "\n",
    " - **$k$-folds**: \n",
    " - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Topics\n",
    "===========\n",
    "\n",
    "**Terminology**\n",
    " - What is **deep learning**: simply: multi-layer neural networks\n",
    " - **Training and Inference**: \"inference\" here just refers to \"testing\".\n",
    " - **Type I vs Type II error**: Type-I is false positive; Type-II is true-negative.\n",
    " - **Gini impurity**: A measure of the fraction of misclassified events in a given region. Used for optimizing (B)DTs\n",
    " - **Entropy**:\n",
    " - **Information gain**:\n",
    " - **Uncertainty** (from a classifier): The probability value assigned to a classified object by the classifier (how sure is the classifier)\n",
    " - **Calibrated** (of a classifier): The probabilities assigned should match reality (think Nate Silver)\n",
    " - **Precision**: $n_\\text{tp}/n_\\text{fp}$ (true positive / false positive test results)\n",
    " - **Recall (sensitivity)**: $n_\\text{tp}/n_\\text{true}$ (true positive / all true elements in the population)\n",
    " - **Accuracy**: The fraction of correct assignments considering *all* classifications (positive *and* negative).\n",
    " - **Balanced Accuracy**: average accuracy per class (to account for imbalanced datasets)\n",
    " - **Confusion matrix**: The matrix with the predicted class on one axis and with true class on the other axis.\n",
    " - **F1 score**: Harmonic mean between the precision and recall (e.g. $\\left(\\frac{p^{-1} + r^{-1}}{2}\\right)^{-1}$). Note that it does not consider false positives, and therefore it is not suitable as a metric for certain problems.\n",
    " - **Brier score**: pretty much a mean squared error for classification problems, e.g. with probabilities for binary classification. $BS = \\frac{1}{N}\\sum_i (r_i - p_i)^2$ and it runs between 0 and 1.\n",
    " - **Exogenous variables**: Input parameters to a model\n",
    " - **Endogenous variables**: Output parameters of a model\n",
    " - **Markov Decision Process**:\n",
    " - **Markov Decision Monte Carlo**:\n",
    " - **One-hot encoding**: A way of encoding categorical data in a bitwise manner, such that only 1 bit can be set to 1. For example, if you have 3 categories, the one-hot encoding of these 3 categories is 001, 010, and 100. Or, you have three variables, so lots of zeros in the data.\n",
    " - **Automatic Differentiation**: This is an algorithmic/computational method of differentiation that repeatedly applies the chain rule of differentiation to calculate derivatives. Typically you can apply the chain rule inside-out (\"forward accumulation\") or outside-in (\"reverse accumulation\" or backpropagation).\n",
    " - **Backpropagation**: A special case of automatic differentation, in which the chain rule is evaluated in an outside-in manner. This allows for a faster computation, at the expense of a larger memory footprint. (For more details, see Wikipedia.)\n",
    "\n",
    "**Regularization**: \n",
    " - Generally speaking, methods that can be used to **reduce overfitting**.\n",
    " - **Ridge** (L2): Introduce a penalty term to the least squares metric to prefer coefficients close to 0. (This can fix the issue of multicollinearity, for instance.)\n",
    " - **Lasso** (L1): Drop input variables (sets the weight equal to zero), effectively reducing the number of inputs.\n",
    " - **Dropout**: Randomly drop nodes of a NN during training to reduce noisy, meaningless nodes.\n",
    " - **Data augmentation**: Increase the size of the training data.\n",
    " - **Early stopping**: stop when the performance starts to get worse.\n",
    "\n",
    "**Multicollinearity** in multiple linear regression: \n",
    " - Effectively introduces a degeneracy in the solution (e.g. `y=ax+b` or `y=dx+b` are equally valid solutions)\n",
    " - Affects the variance of the prediction (?)\n",
    " - May cause the matrix to be non-invertible (less than full-rank tensor)\n",
    " - May occur when your inputs have not been properly studied or cleaned for redundancy\n",
    " - Statistical tests exist to identify these issues\n",
    " - Solutions:\n",
    "   - Remove one of the variables; start simpler.\n",
    "   - Principle Component analysis to orthoganalize your variables\n",
    "\n",
    "**Curse of Dimensionality**: Techniques to avoid it include principle component analysis; clustering, ... ?\n",
    "\n",
    "**What do people mean by the \"Bias vs Variance\" tradeoff**: Generally this is a euphemism for overtraining, or basically picking too many parameters for a fit. A fit with too many parameters will have an unnaturally low bias, but would have a very high variance on a testing data set. This can be avoided using e.g. F-tests (for e.g. a regression), or using techniques to avoid over-fitting the data (see learning curves, validation/test sets, etc.)\n",
    "\n",
    "**What is a generative vs a discriminative model?**\n",
    "   - **Generative models** basically model the joint probability density function ($p(x,y)$); or, they attempt to generate inputs $\\mathbf{x}$ given an output y (like cat $\\rightarrow$ cat image). **Discriminative models** model conditional probabilities ($p(y|x)$); typically slower to converge but eventually outperform. (Note that if no probability model is used, then it is generally considered discriminative.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personal Questions:\n",
    "==========\n",
    "\n",
    "**Question: what's the hardest part about Data Analysis / Machine learning**:\n",
    " - Knowing your inputs is probably the most important element (bad stuff in $\\rightarrow$ bad stuff out)\n",
    " - Knowing which methods to apply in which situations\n",
    "\n",
    "**Introduce yourself. Why are you a good candidate?**\n",
    " - **Experimental** Physicist and data scientist; ATLAS Detector; largest experiment ever built\n",
    " - Huge variety of data science techniques; cutting edge; Petabyte-scale datasets\n",
    " - Collaborate very well with people from around the world; work well in groups\n",
    " - Results-focused; keep the key goals in mind and deliver on strict deadlines.\n",
    " - Passionate about what I do (physics or environmental) - THIS is the field that I am passionate about.\n",
    " - Highy motivated.\n",
    " - My motivation: **Deeply concerned about the environment. With my skill-set, in a unique position to make a difference.**\n",
    " - Believe in the mission of [company].\n",
    "\n",
    "**What are the most important attributes of a data scientist**:\n",
    " - Strategic approach; clear goal in mind.\n",
    " - Effective communication of results (and documentation)\n",
    " - Ability to work in a team (and learn from team members)\n",
    " - Critial problem solving skills; analytic mind\n",
    " - A good toolbox; understanding of the underlying concepts\n",
    " - Experience in mentoring fellow data scientists\n",
    " - Curiosity, care deeply about the domain knowledge\n",
    "\n",
    "**Why do you want to work for our company specifically**:\n",
    " - Reputation in the industry\n",
    " - Clear mission and goals for the future\n",
    " - Want to grow and develop with the company (to keep the job interesting)\n",
    " - Success of the company is good for the planet\n",
    "\n",
    "**How would you contribute to [company] as a data analyst**:\n",
    " - Contributions on day 1\n",
    " - Holistic approach to understanding how I can contribute. Try to focus on the key performance of not only the wind assets, but the profitability of the company (these goals are one in the same).\n",
    "\n",
    "**Question: What is your focus in your current profession**:\n",
    " - Heavy focus on **feature engineering**: a very robust way to run a performant analysis and also understand the underlying mechanisms (which is very important to us)\n",
    " - Machine learning techniques are generally used for an extra boost in performance, but **typically benchmarked against a simpler, robust method**. Penalty for being wrong is high: \"announcing the discovery of a particle that does not exist.\"\n",
    "\n",
    "**Question: What is your experience with time series forecasting**:\n",
    " - Deal regularly with an analagous problem: **data spectra in a particular variable, such as a falling distribution of events** in a particle mass variable, in events generated by a detector. The problem is exactly the same: the data is stochastic, we need to extrapolate our prediction to a region where we don't have data (or we have \"blinded\" our data) and we must have a highly accurate prediction. We basically do **\"anaomaly detection\"** in these distributions. For this we typically use a Gaussian Process.\n",
    " - Build **models** of detector performance (electrical components, thermal performance) to model detector performance over a 14-year lifespan -- so more of a model-building angle and less of a signal processing angle.\n",
    " - Important part of experimental physics is **monitoring detector conditions** with finite state machines and using statistical tests to flag anomalies (one of the first projects I worked on).\n",
    "\n",
    "**Question: what is your biggest weakness**:\n",
    " - Perhaps obvious: I am coming from a **different field**. But of course this can be a strength as well: injecting outside perspective; have seen a huge variety of techniques and solutions; understand the importance of domain knowledge.\n",
    "\n",
    "**My questions for them**:\n",
    " - What is the makeup/organization of the team? How do people generally work in the team - together, separate...?\n",
    " - What machine learning tools do you find yourself using the most in this job?\n",
    " - What other topics besides machine learning are part of the job? How do you typically spend your days?\n",
    " - What is the balance of focus, between engineering/power optimization/fault detection and \"business intelligence?\"\n",
    " - How does the data analysis department fit within the larger structure at [company]?\n",
    " - What computing resources do you have? Computing clusters, cloud computing, etc?\n",
    "\n",
    "**No-No's**:\n",
    " - No: I am leaving physics because there are no permanent jobs. **Yes**: I want to move to a field where I can make a real-world impact on the environment, ...\n",
    " - No: I don't know anything about that. **Yes**: I have done this, in the form of X and Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography\n",
    "================\n",
    " - 'Deep Learning and Its Application to LHC Physics'. https://arxiv.org/pdf/1806.11484.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
