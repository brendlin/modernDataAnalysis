{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Notes on Machine Learning\n",
    "==============\n",
    "\n",
    "Areas of follow-up:\n",
    " - Investigate \"pre-training of initial embeddings with unsupervised learning methods such as autoencoders\" to speed up training (due to the \"vanishing gradient problem\" of deep neural networks)\n",
    " - Would also like to learn more about this: \"Autoencoders attempt to learn a useful layered representation of the data without having to backpropagate through a deep network; standard gradient descent is only used at the end to fine-tune the network.\"\n",
    " - Some interesting work on how you can have a parameterized classifier, that approximates a likelihood ratio, that allows you to estimate the parameters themselves using the classifier. (Baldi P, et al. Eur. Phys. J. C 76:235 (2016))\n",
    "\n",
    "Linear Models for Regression\n",
    "=======\n",
    "\n",
    "Simply put, these are models that try to classify data in a parameter space by applying linear cuts, e.g.\n",
    "\n",
    "$y(\\mathbf x) = a_1 x_1 + a_2 x_2 + .... + b > 0$\n",
    "\n",
    "where the $>0$ threshold indicates a binary classification. For a simple case, take an binary output variable that depends on 2 input variables, e.g. points in a plane. Then this classifier would draw a line in the 2D plane to try to separate the two classes as efficiently as possible.\n",
    "\n",
    "See **support vector machines** for a more complex version of this.\n",
    "\n",
    "\n",
    "Logistic Regression\n",
    "========\n",
    "\n",
    "Used when your output variable is a probability between 0 and 1 (or a multi-class probability with the sum of classes adding to 1). It uses a **logistic (sigmoid) function** to map the inputs to a probability between 0 and 1. A logistic regression can have >1 input variables.\n",
    "\n",
    "The logistic regression is really just a fit to data where y=0 or 1; it can be converted to a classifier by applying a threshold cut to the output logistic estimate.\n",
    "\n",
    "Terminology:\n",
    " - **Logit**: basically the logistic function that maps the infinite space to 0-1.\n",
    " - **Probit**: Very similar function to the logit function, but based on the CDF of a gaussian function.\n",
    " - **Odds ratio**: ??\n",
    "\n",
    "\n",
    "Naive Bayes and Likelihood Ratio\n",
    "========\n",
    "\n",
    "See statisticalMethods -- these assume that your input variables are uncorrelated.\n",
    "\n",
    "\n",
    "Support Vector Machines (SVM)\n",
    "==========\n",
    "\n",
    "A method of regression or classification that maps labeled data onto a larger parameter space so as to maximize the distance between two classes, and uses this mapping as a basis for classification of unlabeled data. \n",
    "\n",
    "In the simplest case, a line divides two classes in plane, or a plane divides two classes in a 3d space. By introducing **kernels** (see statisticalMethods), the boundary can assume complex shapes (which presumably are still hyperplanes in the newly-defined parameter space).\n",
    "\n",
    "The **support vectors** of the SVM are the data points at the boundary of the two different classes, which drive the definition of the boundary itself.\n",
    "\n",
    "SVMs can also be used in the context of clustering.\n",
    "\n",
    "**Scaling / normalization preprocessing** is an important step for SVMs, which otherwise do not do very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees and Forests\n",
    "========\n",
    "\n",
    "Metrics: Gini Impurity and Entropy\n",
    "-------\n",
    "\n",
    "The **Gini impurity** is a decision-tree metric; it is the (summed) probability of misclassifying a sample in a given region. For a multi-class problem, in a given region, it is given by:\n",
    "\n",
    "$I_G(p) = \\sum^J_{i=1}\\left( p_i \\sum_{k\\neq i} p_k\\right) = ... = 1 - \\sum p_k^2 $\n",
    "\n",
    "(as you can see it is one minus the square of the probability of correct assignment.)\n",
    "\n",
    "\n",
    "\n",
    "According to (??), **Gini impurity** is used for classification problems, while **entropy** is used for exploratory analyses (??)\n",
    "\n",
    "Decision Tree\n",
    "--------\n",
    "\n",
    "Find the most powerful variable and apply the most discriminating cut; in the two subsequent populations, repeat the process with the next-most-powerful variable, ... etc. You can **prune** (remove nodes at the end) or **pre-prune** (stopping the continuation of a tree).\n",
    " - **Pruning**: `max_depth`, `max_leaf_nodes`, `min_samples_leaf`\n",
    " - **Feature importance**: A ranked list of the most powerful features (a number between 0 and 1). Note that if there are two highly correlated variables then it is possible that one of the features has a lower feature imporantance than you might think.\n",
    " - **No preprocessing necessary**: Normalization and standardization are not required!\n",
    " \n",
    "\n",
    "Random Forest (a case of Bootstrap Aggregation or BAgging)\n",
    "---------\n",
    "\n",
    "To correct for **over-fitting** in a single decision tree, create an ensemble of trees and insert some randomness into their creation (drop data points; remove variables). Then average the collection of results.\n",
    "\n",
    " - `n_estimators`: Number of trees\n",
    " - `max_features`: Maximum features to look at in the dataset, for a given tree.\n",
    "\n",
    "Boosted Decision Tree (e.g. Gradient Boosted)\n",
    "---------\n",
    "\n",
    " - **Not good for sparse, high-dimensional data**\n",
    " - Generally faster execution than random forests\n",
    " - `learning_rate`: steepness of the gradient descent.\n",
    " - `n_estimators`: Number of trees\n",
    " - `max_depth` (of individual trees): Usually keep this at 5 or lower.\n",
    "\n",
    "AdaBoost\n",
    "----------\n",
    "\n",
    "Achieves the boosting by means of increasing the **weight** of misclassified events to boost their importance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks\n",
    "==========\n",
    "\n",
    "Multilayer Perceptron (MLP)\n",
    "---------\n",
    "\n",
    "A combination of **hidden layers** connected by **weights**, linearly combining inputs and applying an **activation function**.\n",
    "\n",
    "Activation functions:\n",
    " - **Rectified Linear Unit (RELU)**\n",
    " - **tanh** (hyperbolic tangent)\n",
    "\n",
    "**Regularization**: you can regularize a NN by constraining the weights to be close to 0.<br>\n",
    "**Scaling**: Neural nets work better if you scale / normalize the data.\n",
    "\n",
    "\n",
    "\n",
    "Convolutional Neural Networks\n",
    "----------\n",
    " - Convolutional architecture: a filter window is swept over the (spatially-related) input\n",
    " - Pooling: that filtered information is summarized (e.g. takes the maximum or the average)\n",
    " - Normalization Layers\n",
    " - Residual Layers\n",
    " - Graph convolutional networks (as opposed to tree networks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks\n",
    "-----------\n",
    "\n",
    " - long-short-term-memory (LSTM)\n",
    " - Gated recurrent unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adversarial Neural Networks\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation and Metrics\n",
    "============\n",
    "\n",
    "Loss Functions\n",
    "--------\n",
    " - Hinge Loss (???)\n",
    "\n",
    "\n",
    "Metrics\n",
    "-------\n",
    " - **F1**: The **harmonic mean** of the precision and recall\n",
    " - **Accuracy**:\n",
    " - **Confusion Matrix**:\n",
    "\n",
    "\n",
    "Cross-validation\n",
    "-------\n",
    "\n",
    " - **$k$-folds**: \n",
    " - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Topics\n",
    "===========\n",
    "\n",
    "**Terminology**\n",
    " - What is **deep learning**: simply: multi-layer neural networks\n",
    " - **Training and Inference**: \"inference\" here just refers to \"testing\".\n",
    " - **Type I vs Type II error**: Type-I is false positive; Type-II is false-negative.\n",
    " - **Gini impurity**: A measure of the fraction of misclassified events in a given region. Used for optimizing (B)DTs\n",
    " - **Entropy**:\n",
    " - **Information gain**:\n",
    " - **Brier score**:\n",
    " - **Uncertainty** (from a classifier): The probability value assigned to a classified object by the classifier (how sure is the classifier)\n",
    " - **Calibrated** (of a classifier): The probabilities assigned should match reality (think Nate Silver)\n",
    " - **Precision**: $n_\\text{tp}/n_\\text{fp}$ (true positive / false positive test results)\n",
    " - **Recall (sensitivity)**: $n_\\text{tp}/n_\\text{true}$ (true positive / all true elements in the population)\n",
    " - **F1 score**: Harmonic mean between the precision and recall (e.g. $\\left(\\frac{p^{-1} + r^{-1}}{2}\\right)^{-1}$)\n",
    " - **Brier score**: pretty much a mean squared error for classification problems, e.g. with probabilities for binary classification. $BS = \\frac{1}{N}\\sum_i (r_i - p_i)^2$ and it runs between 0 and 1.\n",
    "\n",
    "**Multicollinearity** in multiple linear regression: \n",
    " - Effectively introduces a degeneracy in the solution (e.g. `y=ax+b` or `y=dx+b` are equally valid solutions)\n",
    " - Affects the variance of the prediction (?)\n",
    " - May cause the matrix to be non-invertible (less than full-rank tensor)\n",
    " - May occur when your inputs have not been properly studied or cleaned for redundancy\n",
    " - Statistical tests exist to identify these issues\n",
    " - Solutions:\n",
    "   - Remove one of the variables; start simpler.\n",
    "   - Principle Component analysis to orthoganalize your variables\n",
    "\n",
    "**Curse of Dimensionality**: Techniques to avoid it include principle component analysis; clustering, ... ?\n",
    "\n",
    "**What do people mean by the \"Bias vs Variance\" tradeoff**: Generally this is a euphemism for overtraining, or basically picking too many parameters for a fit. A fit with too many parameters will have an unnaturally low bias, but would have a very high variance on a testing data set. This can be avoided using e.g. F-tests (for e.g. a regression), or using techniques to avoid over-fitting the data (see learning curves, validation/test sets, etc.)\n",
    "\n",
    "**What is a generative vs a discriminative model?**\n",
    "   - **Generative models** basically model the joint probability density function ($p(x,y)$); or, they attempt to generate inputs $\\mathbf{x}$ given an output y (like cat $\\rightarrow$ cat image). **Discriminative models** model conditional probabilities ($p(y|x)$); typically slower to converge but eventually outperform. (Note that if no probability model is used, then it is generally considered discriminative.)\n",
    "\n",
    "**Question: what's the hardest part about Machine learning**:\n",
    " - Knowing your inputs is probably the most important element (bad stuff in $\\rightarrow$ bad stuff out)\n",
    " - Knowing which methods to apply in which situations\n",
    "\n",
    "**Question: what is your biggest weakness**:\n",
    " - Perhaps obvious: I am coming from a **different field**. But of course this can be a strength as well: injecting outside perspective; have seen a huge variety of techniques and solutions; understand the importance of domain knowledge.\n",
    "\n",
    "**My questions for them**:\n",
    " - What is the makeup/organization of the team? How do people generally work in the team - together, separate...?\n",
    " - What is the balance of focus, between engineering/power optimization and \"business intelligence?\"\n",
    " - What machine learning tools do you find yourself using the most in this job?\n",
    " - What other topics besides machine learning are part of the job? How do you typically spend your days?\n",
    "\n",
    "**Regularization**: \n",
    " - Generally speaking, methods that can be used to **reduce overfitting**.\n",
    " - **Ridge** (L2): Introduce a penalty term to the least squares metric to prefer coefficients close to 0. (This can fix the issue of multicollinearity, for instance.)\n",
    " - **Lasso** (L1): Drop input variables (sets the weight equal to zero), effectively reducing the number of inputs.\n",
    " - **Dropout**: Randomly drop nodes of a NN during training to reduce noisy, meaningless nodes.\n",
    " - **Data augmentation**: Increase the size of the training data.\n",
    " - **Early stopping**: stop when the performance starts to get worse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography\n",
    "================\n",
    " - 'Deep Learning and Its Application to LHC Physics'. https://arxiv.org/pdf/1806.11484.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
