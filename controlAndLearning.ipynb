{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "462d9865",
   "metadata": {},
   "source": [
    "Markov Chain\n",
    "------\n",
    "\n",
    "A **Markov process** is a stochastic process in which the state of the next timestep (or element) depends only on the current state of the process, e.g. the next state is independent of the entire history, save for the last state. Put in other terms, a prediction modeling a Markov process will perform the same regardless of whether it knows about the history of the Markov Chain (save for the current state). A **Markov chain** is usually defined as a Markov process with discrete timesteps, though this can vary.\n",
    "\n",
    "\n",
    "Markov Decision Process\n",
    "------\n",
    "\n",
    "\n",
    "\n",
    "Dynamic Programming\n",
    "------\n",
    "\n",
    "Dynamic programming refers to the process of taking a complicated problem and breaking it into smaller, often recursive sub-problems. **Richard Bellman** made many contributions to this area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c5e22",
   "metadata": {},
   "source": [
    "Reinforcement Learning\n",
    "========\n",
    "\n",
    " - In general: balancing between **exploration** and **current knowledge**.\n",
    " - Here, there is no labeled data required (unsupervised learning).\n",
    " - Compared to **dynamic programming techniques**, no underlying mathematical model of the problem is assumed (as the models are typically too complex)\n",
    " - The **policy** is often referred to as $\\pi$\n",
    " - The agent can have full access to the state (full observability) or only partial observability (in the case where there is e.g. noise).\n",
    " - To deal with large environments, **function approximation** is a useful tool.\n",
    " \n",
    "Concepts\n",
    "------\n",
    " - $\\epsilon$-greedy method: Exploitation is chosen with probability $(1-\\epsilon)$, and the best long-term effect is sought.\n",
    "   - Ties between actions are broken uniformly at random .... ?\n",
    " - **Stationary policy**: the action taken depends only on the last state visited.\n",
    " - **Deterministic stationary policy**: deterministically determines the action based on the current state.\n",
    "\n",
    "Different Approaches:\n",
    "-------\n",
    " - **Value Function Approximation**: This approach maintains a set of estimates for the expected returns of a given policy (either the current policy, or the supposed optimal policy).\n",
    "   - **Monte Carlo Method**: Basically you sample the state-action state and over a significant amount of samples, you develop an estimate of the function values $Q^\\pi(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eedb73",
   "metadata": {},
   "source": [
    "Q Learning and Deep-Q Learning\n",
    "======="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe07572",
   "metadata": {},
   "source": [
    "Policy Gradient Methods\n",
    "=======\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77beae8",
   "metadata": {},
   "source": [
    "Actor-critic methods\n",
    "=====\n",
    "\"Actor critic methods are a hybrid between Q Learning and Policy gradient methods.\" ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8b86cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
