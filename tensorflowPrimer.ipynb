{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details about the definitions below:\n",
    "========\n",
    "\n",
    "Gloassary of Terms\n",
    "----------\n",
    "**Sample, Batch, and Epoch**: A *sample* is defined as one element (event) of a dataset. A *batch* is a set of samples, after which the weights are updated (e.g. after backpropagation is performed). An *epoch* is generally defined as one full pass over the dataset.\n",
    "\n",
    "**Static vs Dynamic models**: A **static** model is one in which training is performed before deployment. A **dynamic** model is one in which the model is continuously updated while in use.\n",
    "\n",
    "**tf.GradientTape**: This is a way to trace matrix operations for computing gradients later. Seems like it is more important for very low-level programming in TensorFlow, but interesting nonetheless to understand how things are working.\n",
    "\n",
    "**Trainable variables**: The designation of a `trainable=True/False` variable determines whether a variable will need to be differentiated, e.g. during backpropagation. Some obeserver variables, such as a training step counter, do not need to be trainable.\n",
    "\n",
    "**tf.data.Dataset**:\n",
    "\n",
    "**tf.cast**:\n",
    "\n",
    "**tf.newaxis**:\n",
    "\n",
    "- **`keras.models.Sequential`**: A sequence of layers\n",
    " - **`keras.layers.Flatten`**: Flatten a ndarry into a 1-dimensional array\n",
    " - **`keras.layers.Dense`**: A dense neural network layer\n",
    " - **`model.evaluate(test_data)`** vs **`model.predict(test_data)`**: Predict will output the prediction (e.g. the final output estimate of *y* (e.g. the output is the vector *y*), whereas evaluate will calculate the loss function and any metrics (the output is a vector of loss function + these metrics).\n",
    "\n",
    "**Optimizer**:\n",
    " - `'adam'`: \n",
    " - `'sgd'`: Stochastic Gradient Descent\n",
    " - keras.optimizers.RMSprop(lr=0.001): The learning rate is adapted to each of the free parameters, based on a rolling average of the magnitudes of recent gradients for that rate.\n",
    "\n",
    "**Loss**: The loss function to minimize. Options are:\n",
    " - `'mean_squared_error'` (like it sounds)\n",
    " - `Huber Loss Function`: ???\n",
    "\n",
    "**Metrics**: These are quantities that are calculated alongside the evaluation, for your convenience. You can add as many of these as you like. Examples:\n",
    " - Probabilistic metrics:\n",
    "   - XXXCrossEntropy: \n",
    "   - KLDivergence:\n",
    "   - Poisson:\n",
    " - Regression metrics:\n",
    "   - MeanSquaredError (and variations)\n",
    "   - MeanSquaredLogarithmicError\n",
    "   \n",
    "**Callbacks**: During model fitting, a callback is run after each epoch to e.g. stop the training once the descired accuracy is reached.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Front Matter\n",
    "========\n",
    "\n",
    "This is a quick notebook to explore the outputs of the Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%load_ext tensorboard\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an example dataset\n",
    "========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nPoints = 1000\n",
    "x_sig = 1 * np.random.randn(nPoints) + 1\n",
    "y_sig = 1 * np.random.randn(nPoints) + 1\n",
    "x_bkg = 1 * np.random.randn(nPoints*10) - 1\n",
    "y_bkg = 1 * np.random.randn(nPoints*10) - 1\n",
    "label = np.append(np.ones(len(x_sig)),np.zeros(len(x_bkg))).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(np.append(x_sig,x_bkg),\n",
    "                           np.append(y_sig,y_bkg),\n",
    "                           label)),columns=['x','y','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df,vars=['x', 'y'],hue='label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot correlations of variables\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sns.heatmap(df[df['label'] == 1][['x','y']].corr(),annot=True,vmin=-1,vmax=1)\n",
    "b, t = plt.ylim() # discover the values for bottom and top\n",
    "plt.ylim(b + 0.5, t - 0.5) # update the ylim(bottom, top) values\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the Keras Model, compile and fit\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "#    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(5, name='first_dense', input_shape=(2,), activation='relu'),\n",
    "    tf.keras.layers.Dense(5, name='middle_dense', activation='relu'),\n",
    "#    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, name='last_dense', activation='sigmoid') # sigmoid for binary; softmax for multi?\n",
    "    ])\n",
    "\n",
    "##### TODO:\n",
    "#probability_model = tf.keras.Sequential([\n",
    "#  model,\n",
    "#  tf.keras.layers.Softmax()\n",
    "#])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(df[['x','y']].values,\n",
    "                    df['label'].values,\n",
    "                    batch_size=64,\n",
    "                    epochs=20,\n",
    "                    verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.params)\n",
    "print(history.history.keys())\n",
    "print(list('%.3f'%a for a in history.history['accuracy']))\n",
    "#dir(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the Model Structure\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pydot as pyd\n",
    "#import graphviz\n",
    "#tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "#    rankdir='TB', expand_nested=True, dpi=96)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1\n",
    "# from IPython.display import SVG\n",
    "# SVG(tf.keras.utils.model_to_dot(model,expand_nested=True).create(prog='dot', format='svg'))\n",
    "# version 2\n",
    "#tf.keras.utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True,expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance\n",
    "==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.metrics_names)\n",
    "model.evaluate(df[['x','y']].values,df['label'].values,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This sets things as 1 or 0\n",
    "# based on whether or not is is above or below 0.5\n",
    "df['binary_prediction'] = model.predict_classes(df[['x','y']].values)\n",
    "# This is the continuous variable\n",
    "df['discriminant'] = model.predict(df[['x','y']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(df['binary_prediction'].values,df['label'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the raw discriminants\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[df['label'] ==0]['discriminant'],bins=20,label='bkg',density=True,histtype='step',lw=2)\n",
    "plt.hist(df[df['label'] ==1]['discriminant'],bins=20,label='sig',density=True,histtype='step',lw=2)\n",
    "plt.legend()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC Curve\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(df['label'].values, df['discriminant'].values)\n",
    "from sklearn.metrics import auc\n",
    "auc_keras = auc(fpr_keras, tpr_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "plt.ylabel('signal efficiency (true positive rate)')\n",
    "plt.xlabel('bkg efficiency (false positive rate)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-D Gradients\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since probability is a continuous value from 0 to 1, we are getting many contours.\n",
    "#If your visualization is restricted to 2 classes (output is 2D softmax vector) you can use this simple code\n",
    "\n",
    "def plotModelOut(x,y,model):\n",
    "    '''\n",
    "    x,y: 2D MeshGrid input\n",
    "    model: Keras Model API Object\n",
    "    '''\n",
    "    grid = np.stack((x,y))\n",
    "    grid = grid.T.reshape(-1,2)\n",
    "    outs = model.predict(grid)\n",
    "    y1 = outs.T[0].reshape(x.shape[0],x.shape[0])\n",
    "    plt.contourf(x,y,y1,cmap='binary')\n",
    "    plt.colorbar()\n",
    "\n",
    "    #plt.show()\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(-4,4,100), np.linspace(-4,4,100))\n",
    "plotModelOut(xx,yy,model)\n",
    "\n",
    "plt.scatter(df['x'][df['label'] == 0],df['y'][df['label'] == 0])\n",
    "plt.scatter(df['x'][df['label'] == 1],df['y'][df['label'] == 1])\n",
    "\n",
    "plt.xlim([-4,4])\n",
    "plt.ylim([-4,4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hello World of Neural Networks\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(units=1, input_shape=[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0],dtype=float)\n",
    "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0],dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xs, ys, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict([10.0,11.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST Dataset\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.shape,test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data:\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "test_images  = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images[0])\n",
    "#print(train_images[0])\n",
    "#print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([keras.layers.Flatten(), # Optional: specifying shape, e.g. input_shape=(28,28)\n",
    "                          keras.layers.Dense(512,activation=tf.nn.relu), # Try 128, 1024, ...\n",
    "                          keras.layers.Dense(256,activation=tf.nn.relu), # Try 128, 1024, ...\n",
    "                          keras.layers.Dense(10,activation=tf.nn.softmax)]) # 10 should match the output type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= tf.optimizers.Adam(),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a callback to our fitting step in order to sto \n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('accuracy')>0.6):\n",
    "      print(\"\\nReached 60% accuracy so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "callback = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_images,train_labels,epochs=5,callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_images,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These predictions are the scores for each of the 10 categories, for the first test image.\n",
    "# As you can see, \"9\" is the winner.\n",
    "classifications = model.predict(test_images)\n",
    "print(classifications[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handwriting MNIST\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: train_mnist\n",
    "def train_mnist():\n",
    "    # Please write your code only where you are indicated.\n",
    "    # please do not remove # model fitting inline comments.\n",
    "\n",
    "    # YOUR CODE SHOULD START HERE\n",
    "    class myCallback(tf.keras.callbacks.Callback):\n",
    "      def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy')>=0.99):\n",
    "          print(\"\\nReached 99% accuracy so cancelling training!\")\n",
    "          self.model.stop_training = True\n",
    "\n",
    "    callback = myCallback()\n",
    "    # YOUR CODE SHOULD END HERE\n",
    "\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "\n",
    "    (x_train, y_train),(x_test, y_test) = mnist.load_data(path='mnist.npz')\n",
    "    # YOUR CODE SHOULD START HERE\n",
    "    x_train = x_train / 255.0 # normalize the data to between 0 and 1\n",
    "    x_test  = x_test / 255.0 # normalize the data to between 0 and 1\n",
    "    # YOUR CODE SHOULD END HERE\n",
    "    model = tf.keras.models.Sequential([\n",
    "        # YOUR CODE SHOULD START HERE\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(128,activation=tf.nn.relu),\n",
    "        keras.layers.Dense(10,activation=tf.nn.softmax)\n",
    "        # YOUR CODE SHOULD END HERE\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # model fitting\n",
    "    history = model.fit(# YOUR CODE SHOULD START HERE\n",
    "        x_train,y_train,epochs=11,callbacks=[callback]\n",
    "              # YOUR CODE SHOULD END HERE\n",
    "    )\n",
    "    # model fitting\n",
    "    return history.epoch, history.history['accuracy'][-1]\n",
    "\n",
    "train_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data(path='mnist.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[0])\n",
    "plt.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks\n",
    "----------\n",
    "\n",
    "**Note that the Coursera people wanted you to run this using GPUs in Colab. If you run it locally, your computer's fan starts running.**\n",
    "\n",
    " - The `CONV2D` layer runs a bunch of filters (the number of filters is specified by the first argument) on top of the \n",
    "   - Note that specifying `'relu'` will toss out negative numbers. Interesting.\n",
    " - The `MaxPool2D` layer finds the maximum value in a window.\n",
    " \n",
    "An interesting idea I heard was to **start with not so many filters in the first Conv2D layer**, and then as you continue to pool (and therefore reduce the size of the image) you should **add more filters to learn in later Conv2D layers.**. Therefore, the number of parameters does not explode, and you have more opportunities for discovering interesting features in the final convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "test_loss = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(3,4,figsize=(12,8))\n",
    "\n",
    "FIRST_IMAGE=0\n",
    "SECOND_IMAGE=7\n",
    "THIRD_IMAGE=26\n",
    "CONVOLUTION_NUMBER = 3\n",
    "\n",
    "from tensorflow.keras import models\n",
    "\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
    "\n",
    "for x in range(0,4):\n",
    "    f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "    axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "    axarr[0,x].grid(False)\n",
    "    f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "    axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "    axarr[1,x].grid(False)\n",
    "    f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "    axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "    axarr[2,x].grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment: Convolutional Neural Networks and (handwriting) MNIST\n",
    "----------\n",
    "\n",
    "Very similar to what you did above. Some interesting code snippet though:\n",
    "\n",
    "```python\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "```\n",
    "\n",
    "Something to look into.... interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: train_happy_sad_model\n",
    "def train_happy_sad_model():\n",
    "    # Please write your code only where you are indicated.\n",
    "    # please do not remove # model fitting inline comments.\n",
    "\n",
    "    DESIRED_ACCURACY = 0.999\n",
    "\n",
    "    class myCallback(tf.keras.callbacks.Callback):\n",
    "      def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy')>0.999):\n",
    "          print(\"\\nReached 99.9% accuracy so cancelling training!\")\n",
    "          self.model.stop_training = True\n",
    "         # Your Code\n",
    "\n",
    "    callbacks = myCallback()\n",
    "    \n",
    "    # This Code Block should Define and Compile the Model. Please assume the images are 150 X 150 in your implementation.\n",
    "    model = tf.keras.models.Sequential([\n",
    "        # Your Code Here\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    #model.summary()\n",
    "    #return\n",
    "    \n",
    "    from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "    model.compile(optimizer=RMSprop(lr=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "        \n",
    "    # This code block should create an instance of an ImageDataGenerator called train_datagen \n",
    "    # And a train_generator by calling train_datagen.flow_from_directory\n",
    "\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "    train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "    # Please use a target_size of 150 X 150.\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        '/tmp/h-or-s',\n",
    "        target_size=(150, 150),\n",
    "        class_mode='binary')\n",
    "        # Your Code Here)\n",
    "    # Expected output: 'Found 80 images belonging to 2 classes'\n",
    "    \n",
    "    # This code block should call model.fit_generator and train for\n",
    "    # a number of epochs.\n",
    "    # model fitting\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=10,\n",
    "        epochs=15,\n",
    "        verbose=1,\n",
    "        callbacks=[callbacks])\n",
    "          # Your Code Here)\n",
    "    # model fitting\n",
    "    return history.history['acc'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some cool code to visualize intermediate Convolutional Layers:\n",
    "--------\n",
    "\n",
    "```python\n",
    "import numpy as np  \n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "# Let's define a new Model that will take an image as input, and will output\n",
    "# intermediate representations for all layers in the previous model after\n",
    "# the first.\n",
    "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
    "#visualization_model = Model(img_input, successive_outputs)\n",
    "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
    "# Let's prepare a random input image from the training set.\n",
    "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
    "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
    "img_path = random.choice(horse_img_files + human_img_files)\n",
    "\n",
    "img = load_img(img_path, target_size=(300, 300))  # this is a PIL image\n",
    "x = img_to_array(img)  # Numpy array with shape (150, 150, 3)\n",
    "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n",
    "\n",
    "# Rescale by 1/255\n",
    "x /= 255\n",
    "\n",
    "# Let's run our image through our network, thus obtaining all\n",
    "# intermediate representations for this image.\n",
    "successive_feature_maps = visualization_model.predict(x)\n",
    "\n",
    "# These are the names of the layers, so can have them as part of our plot\n",
    "layer_names = [layer.name for layer in model.layers[1:]]\n",
    "\n",
    "# Now let's display our representations\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "    if len(feature_map.shape) == 4:\n",
    "        # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
    "        n_features = feature_map.shape[-1]  # number of features in feature map\n",
    "        # The feature map has shape (1, size, size, n_features)\n",
    "        size = feature_map.shape[1]\n",
    "        # We will tile our images in this matrix\n",
    "        display_grid = np.zeros((size, size * n_features))\n",
    "        for i in range(n_features):\n",
    "            # Postprocess the feature to make it visually palatable\n",
    "            x = feature_map[0, :, :, i]\n",
    "            x -= x.mean()\n",
    "            x /= x.std()\n",
    "            x *= 64\n",
    "            x += 128\n",
    "            x = np.clip(x, 0, 255).astype('uint8')\n",
    "            # We'll tile each filter into this big horizontal grid\n",
    "            display_grid[:, i * size : (i + 1) * size] = x\n",
    "        # Display the grid\n",
    "        scale = 20. / n_features\n",
    "        plt.figure(figsize=(scale * n_features, scale))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation\n",
    "=========\n",
    "\n",
    "Image data can be augmented using parameters in the ImageDataGenerator:\n",
    "\n",
    "```python\n",
    "# This code has changed. Now instead of the ImageGenerator just rescaling\n",
    "# the image, we also rotate and do other operations\n",
    "# Updated to do image augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "```\n",
    "\n",
    "Transfer Learning\n",
    "=========\n",
    "\n",
    " - `include_top`: whether to include a fully-connected layer at the top of the model, or just skip directly to the convolutions\n",
    " - The weights file with 'notop' will be loaded -- note that this kind of goes with `weights=None` for the pre-trained model.\n",
    " - Finally, we set the pre-trained model layers to be non-trainable.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
    "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
    "  \n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "pre_trained_model = InceptionV3(input_shape = (150, 150, 3), \n",
    "                                include_top = False, \n",
    "                                weights = None)\n",
    "\n",
    "pre_trained_model.load_weights(local_weights_file)\n",
    "\n",
    "for layer in pre_trained_model.layers:\n",
    "  layer.trainable = False\n",
    "  \n",
    "# pre_trained_model.summary()\n",
    "\n",
    "last_layer = pre_trained_model.get_layer('mixed7')\n",
    "print('last layer output shape: ', last_layer.output_shape)\n",
    "last_output = last_layer.output\n",
    "```\n",
    "\n",
    "- **Finally, we add layers, starting from the last output -- notice how there is some kind of daisy-chain situation going on here:**\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# Flatten the output layer to 1 dimension\n",
    "x = layers.Flatten()(last_output)\n",
    "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "# Add a dropout rate of 0.2\n",
    "x = layers.Dropout(0.2)(x)                  \n",
    "# Add a final sigmoid layer for classification\n",
    "x = layers.Dense  (1, activation='sigmoid')(x)           \n",
    "\n",
    "model = Model( pre_trained_model.input, x) \n",
    "\n",
    "model.compile(optimizer = RMSprop(lr=0.0001), \n",
    "              loss = 'binary_crossentropy', \n",
    "              metrics = ['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Epochs\n",
    "=========\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switching from Binary to multi-classifier\n",
    "======\n",
    "\n",
    " - The ImageDataGenerator will need `class_mode` to change from \"binary\" to \"categorical\"\n",
    " - The final layer of your Model will need to be `Dense(3,activation='softmax')` (3 for the number of categories!)\n",
    " - In Compiling the model, you should use a loss function of `'categorical_crossentropy'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word-based Encodings\n",
    "=========\n",
    "\n",
    " - Tokenize the words.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
