{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing and Preparing Data\n",
    "=========\n",
    "\n",
    "Topics:\n",
    " - Splitting your data into training and testing (also k-folds) ???\n",
    " - **Principle Component Analysis**\n",
    " - **Non-negative Matrix Factorization (NMF)** ???\n",
    " - **t-SNE** ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing Data\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Other options are StandardScaler... others are available too!\n",
    "\n",
    "# Prepare an example dataset:\n",
    "correlated_part = np.random.normal(0,3,(1000))\n",
    "x0 = np.random.normal(10,1,(1000)) + correlated_part\n",
    "x1 = np.random.normal(10,2,(1000)) + correlated_part\n",
    "\n",
    "# Make the ntuple\n",
    "x_train = np.swapaxes(np.array((x0,x1)),0,1)\n",
    "# x_train.shape\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "\n",
    "# Can also do x_scaled = scaler.fit(x_train).transform(x_train)\n",
    "# or even x_scaled = scaler.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2) = plt.subplots(1,2,figsize=(16, 6))\n",
    "ax1.scatter(x0,x1)\n",
    "ax2.scatter(x_train_scaled[:,0],x_train_scaled[:,1])\n",
    "ax1.set_title('original')\n",
    "ax2.set_title('scaled');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principle Component Analysis (PCA)\n",
    "==========\n",
    "\n",
    "Construct an **orthonormal basis** from the data in which the data points in the sample are **uncorrelated**. The first few components are the components that **maximize the variance** of the data.\n",
    "\n",
    "The principle components are the **eigenvectors** of the data's **covariance matrix**. The principle components can  be computed by either an **eigendecomposition of the covariance matrix**, or a **singular value decomposition** of the data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(x_train_scaled)\n",
    "x_pca = pca.transform(x_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2) = plt.subplots(1,2,figsize=(16, 6))\n",
    "ax1.scatter(x0,x1)\n",
    "ax2.scatter(x_pca[:,0],x_pca[:,1])\n",
    "ax1.set_title('original')\n",
    "ax2.set_title('PCA');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rotational components\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Plotting (simple time series; sns pairplot)\n",
    "=======\n",
    "\n",
    " - Plot vs Time\n",
    " - Plot the variables of the dataset and their correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ninja_pv_wind_profiles_singleindex.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:1000]\n",
    "df = df[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=[20,5])\n",
    "df['time_dt'] = pd.to_datetime(df['time'])\n",
    "ax.plot(np.array(df['time_dt'][:200]),np.array(df['AT_pv_national_current'][:200]),label='AT')\n",
    "ax.plot(np.array(df['time_dt'][:200]),np.array(df['AT_wind_national_current'][:200]),label='AT wind')\n",
    "ax.plot(np.array(df['time_dt'][:200]),np.array(df['BE_pv_national_current'][:200]),label='Belgium solar')\n",
    "ax.plot(np.array(df['time_dt'][:200]),np.array(df['BE_wind_offshore_current'][:200]),label='Belgium wind offshore')\n",
    "ax.plot(np.array(df['time_dt'][:200]),np.array(df['SE_pv_national_current'][:200]),label='Sweden')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AL_pv'] = df['AL_pv_national_current']\n",
    "df['AT_pv'] = df['AT_pv_national_current']\n",
    "df['AT_wind'] = df['AT_wind_national_current']\n",
    "sns.pairplot(df,vars=['AL_pv', 'AT_pv','AT_wind'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test-train splitting - Scikit-learn\n",
    "-------\n",
    "\n",
    "Scikit-learn has an option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split() :\n",
    "    x_train, x_test, y_train, y_test = train_test_split(iris_X, iris_y, test_size=0.33, random_state=42)\n",
    "\n",
    "    # Another option is *stratify*, which allows you to balance your samples to make sure there are\n",
    "    # enough of a certain y output class in each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing in Tensorflow - ImageDataGenerator\n",
    "=========\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenearator(rescale=1/255.)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(300,300), # this resizes each image \"on-the-fly\", not affecting the source data.\n",
    "    batch_size=128, # number of images per batch{P\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = ... #(same as above, but with the validation directory)\n",
    "```\n",
    "\n",
    "Keras Preprocessing img_to_array\n",
    "===========\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "img=image.load_img('dog.jpg', target_size=(150, 150))\n",
    "x=image.img_to_array(img)\n",
    "x_exp=np.expand_dims(x, axis=0)\n",
    "images = np.vstack([x_exp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "print(x_exp.shape)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing Words with Keras\n",
    "=========\n",
    "\n",
    "Notes:\n",
    " - Use the same tokenizer for the test sample and the train sample\n",
    " - Add an \"out-of-vocabulary\" token for a placeholder for out-of-vocabulary words\n",
    " - Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences = [\n",
    "    'i love my dog',\n",
    "    'I, love my cat',\n",
    "    'You love my dog!',\n",
    "    'I think my dog is amazing',\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100,oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)\n",
    "\n",
    "sentences.append('my dog loves my manatee')\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(sequences)\n",
    "\n",
    "padded = pad_sequences(sequences) #Options: padding ('post'), maxlen, truncating ('post')\n",
    "print(padded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
