{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Unsupervised) Clustering Algorithms\n",
    "=========\n",
    "\n",
    "Some things to keep in mind:\n",
    " - Common goal: **Dimensionality Reduction**.\n",
    " - Evaluating performance is tricky, in the sense that the final goal is unclear. Thus, unsupervised algorithms are considered **Exploratory**.\n",
    " - Some metrics in case you do know ground truth:\n",
    "   - **Adjusted rand index (ARI)**: similar to accuracy, a measure of how often the correct assignment is made (requires *truth labels*). *Adjusted*, because it adjusts the score against how you would do by random chance (0: no better than random chance. 1: perfect assignment)\n",
    "   - **Normalized mutual information (NMI)**: Similar idea, but **adjusted for asymetrically-sized (unbalanced) clusters**.\n",
    "\n",
    "**Examples from ATLAS**:\n",
    " - Most obvious is **clustering calorimeter deposits** formed from sprays of particles in our particle detector, using **anti-$k_T$ algorithm** (other algorithms exist, such as Cambridge-Aachen, etc.)\n",
    "\n",
    "Examples:\n",
    " - **k-Means Clustering** (see below)\n",
    " - **Agglomerative Clustering** (see below)\n",
    " - **DBScan** (see below)\n",
    " - **Support Vector Machines (SVM)** ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kMeans Clustering\n",
    "-------\n",
    "\n",
    "Start with $n$ cluster centers:\n",
    " - Assign each data point to the nearest cluster center\n",
    " - Move the cluster center to match the mean of the data points\n",
    " - Repeat the process until stable.\n",
    "\n",
    "Some clear disadvantages regarding data that is nontrivially distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agglomerative Clustering\n",
    "-----------\n",
    "\n",
    "Start with every sample (/event) belonging to its own cluster, then merge \"similar\" clusters until some criteria is achieved. This is a **hierarchical clustering** algorithm, meaning you can form a **dendrogram** \n",
    "\n",
    "Options/methods:\n",
    " - **ward**: merge 2 clusters based on keeping the variance low within a cluster. Clusters are generally equally sized.\n",
    " - **average**: Merge 2 clusters that have the smallest **average** distance between all points.\n",
    " - **complete**: Merge 2 clusters that have the smallest **maximum** distance between their points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spacial Clustering for Applications with Noise)\n",
    "--------\n",
    "\n",
    "Number of clusters can be *a priori* unknown.\n",
    "\n",
    "Description / options:\n",
    " - `min_samples`: the minimum number of samples within a distance metric `eps`\n",
    " - `eps`: the distance metric.\n",
    " - Core samples are built and grown; adjacent core samples closer than `eps` are merged.\n",
    " - If a given sample is not close enough to `min_samples` within distance `eps` then it is labeled as noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
